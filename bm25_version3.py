import os
import json
import re
from typing import List, Dict, Optional
from flask import session

chat_history = []

try:
    import fitz  # PyMuPDF
except Exception as exc:  # pragma: no cover
    raise RuntimeError("PyMuPDF (fitz) is required. Please install with: pip install pymupdf") from exc


def _ensure_deps():
    """Ensure optional dependencies are available; raise helpful errors otherwise."""
    try:
        from rank_bm25 import BM25Okapi  # noqa: F401
    except Exception as exc:  # pragma: no cover
        raise RuntimeError("rank-bm25 is required. Please install with: pip install rank-bm25") from exc

    # jieba is optional; we fall back to regex if unavailable
    try:
        import jieba  # noqa: F401
    except Exception:
        pass


_ensure_deps()
from rank_bm25 import BM25Okapi


def default_tokenize(text: str) -> List[str]:
    """Tokenize text for BM25 scoring.

    Prefer jieba for Chinese segmentation; otherwise, use a regex that splits
    CJK characters to single-char tokens and Latin to word tokens.
    """
    try:
        import jieba
        return [token.strip() for token in jieba.cut(text) if token.strip()]
    except Exception:
        # Keep alphanumerics as words, split CJK into single characters
        return re.findall(r"[\u4e00-\u9fff]|[A-Za-z0-9]+", text)


class BM25DocumentRetriever:
    """Builds a BM25 index at the document (per-PDF) level and retrieves top documents.

    - Each PDF file becomes one document.
    - BM25 is built over tokenized full-document texts.
    - Use search() to get the most relevant documents for a query.
    """

    def __init__(
        self,
        pdf_folder: str = "./ntu_rules_pdfs",
        corpus_path: str = "bm25_docs_big.json",
        tokenizer=default_tokenize,
    ) -> None:
        self.pdf_folder = pdf_folder
        self.corpus_path = corpus_path
        self.tokenize = tokenizer

        self.documents: List[Dict[str, str]] = []  # [{doc_id, text}]
        self._bm25: Optional[BM25Okapi] = None

    def build_or_load_corpus(self) -> None:
        """Load existing corpus JSON or build from PDFs and save it."""
        if os.path.exists(self.corpus_path):
            with open(self.corpus_path, "r", encoding="utf-8") as f:
                self.documents = json.load(f)
            return

        documents: List[Dict[str, str]] = []
        for filename in os.listdir(self.pdf_folder):
            if not filename.lower().endswith(".pdf"):
                continue
            pdf_path = os.path.join(self.pdf_folder, filename)
            try:
                doc = fitz.open(pdf_path)
            except Exception:
                # Skip unreadable PDFs but keep going
                continue
            text_fragments: List[str] = []
            for page in doc:
                try:
                    text_fragments.append(page.get_text())
                except Exception:
                    pass
            full_text = "\n".join(text_fragments).strip()
            if not full_text:
                continue
            documents.append({"doc_id": filename, "text": full_text})

        # Persist corpus
        with open(self.corpus_path, "w", encoding="utf-8") as f:
            json.dump(documents, f, ensure_ascii=False)

        self.documents = documents

    def build_index(self) -> None:
        """Build BM25 index for the loaded corpus."""
        if not self.documents:
            self.build_or_load_corpus()
        tokenized_docs = [self.tokenize(d["text"]) for d in self.documents]
        self._bm25 = BM25Okapi(tokenized_docs)

    def search(self, query: str, k: int = 5) -> List[Dict[str, object]]:
        """Return top-k most relevant documents by BM25 score.

        Response schema per item:
        - doc_id: str (PDF filename)
        - score: float (BM25 score)
        - text: str (full document text)
        """
        if self._bm25 is None:
            self.build_index()
        tokenized_query = self.tokenize(query)
        scores = self._bm25.get_scores(tokenized_query)
        ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]

        results: List[Dict[str, object]] = []
        for idx in ranked_indices:
            doc = self.documents[idx]
            results.append({
                "doc_id": doc["doc_id"],
                "score": float(scores[idx]),
                "text": doc["text"],
            })
        return results

    def build_context(self, query: str, k: int = 3, max_chars_per_doc: Optional[int] = 6000) -> str:
        """Build a concatenated context from top-k documents with optional per-doc trimming.

        This is useful if you want to pass entire documents (or trimmed versions) to an LLM.
        """
        top_docs = self.search(query, k=k)
        formatted_docs: List[str] = []
        for rank, item in enumerate(top_docs, start=1):
            text = item["text"]
            if isinstance(max_chars_per_doc, int) and max_chars_per_doc > 0:
                text = text[:max_chars_per_doc]
            header = f"[Document {rank}] {item['doc_id']} (score={item['score']:.3f})"
            formatted_docs.append(f"{header}\n{text}")
        return "\n\n---\n\n".join(formatted_docs)
    

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import json
import re

# Try to ensure BM25 is available
try:
    from rank_bm25 import BM25Okapi
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "rank-bm25"])  # silent install
    from rank_bm25 import BM25Okapi

# Simple tokenizer that works for Chinese and Latin text
try:
    import jieba
    def tokenize(text: str):
        return [t for t in jieba.cut(text) if t.strip()]
except Exception:
    def tokenize(text: str):
        return re.findall(r"[\u4e00-\u9fff]|[A-Za-z0-9]+", text)

MODEL = None
TOKENIZER = None

# è¼‰å…¥æ¨¡å‹
model_id = "meta-llama/Llama-3.1-8B-Instruct"
#model_id = "google/gemma-3-12b-it"

# é¸æ“‡è£ç½®ï¼ˆCUDAï¼ŒApple MPSï¼ŒCPUï¼‰
if torch.cuda.is_available():
    device = "cuda"
elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

# æ¨¡å‹å¿«å–
if MODEL is None or TOKENIZER is None:
    print("Loading model and tokenizer (this may take a few minutes the first time)...")
    TOKENIZER = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    MODEL = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16 if device in ("cuda", "mps") else torch.float32
    ).to(device)
else:
    print("Using cached model and tokenizer.")

tokenizer = TOKENIZER
model = MODEL

# å»ºç«‹æ¨ç†ç®¡ç·šï¼ˆOptionalï¼‰
llm = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=False)



def _extract_titles_from_context(context: str):
    # ä¾‹ï¼š"[Document 1] æŸæŸè¦ç« .pdf (score=0.812)"
    import re
    return re.findall(r"\[Document\s+\d+\]\s*(.*?)\s*\(score=", context or "")



def generate_prompt(user_input, context, conversation_history):
    if not isinstance(conversation_history, list):
        conversation_history = []
    cleaned_history = []
    for turn in conversation_history:
        if isinstance(turn, dict) and "user" in turn and "assistant" in turn:
            cleaned_history.append({"user": str(turn["user"]), "assistant": str(turn["assistant"])})
        elif isinstance(turn, (list, tuple)) and len(turn) == 2:
            cleaned_history.append({"user": str(turn[0]), "assistant": str(turn[1])})
        else:
            continue
    #titles = _extract_titles_from_context(context)
    print(context)

    print("ã€å­¸ç”Ÿå•é¡Œã€‘", user_input)
    history_str = ""
    for turn in cleaned_history:
        history_str += f"ä½¿ç”¨è€…ï¼š{turn['user']}\næ³•è¦åŠ©ç†ï¼š{turn['assistant']}\n"
    print("ã€å°è©±æ­·å²ã€‘", history_str if history_str else "ç„¡")
    return f"""ä½ æ˜¯ä¸€ä½å°å¤§å­¸ç”Ÿçš„æ³•è¦åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å­¸ç”Ÿçš„å•é¡Œã€‚

[å°è©±æ­·å²]
{history_str}

[æ³•è¦è³‡æ–™]
{context}

[å­¸ç”Ÿå•é¡Œ]
{user_input}

è«‹çµ¦å‡ºæº–ç¢ºã€æ¸…æ¥šçš„å›è¦†ï¼Œè‹¥è³‡æ–™ä¸è¶³ï¼Œè«‹èªªæ˜é‚„éœ€è¦å“ªäº›å­¸ç”Ÿè³‡è¨Šã€‚å›ç­”è¦ç°¡æ½”ï¼Œè‹¥æ³•è¦è³‡æ–™ä¸­æœ‰è·Ÿå­¸ç”Ÿå•é¡Œç„¡é—œçš„è«‹å¿½ç•¥ï¼Œä¸€å®šä¸ç”¨å¤šé¤˜çš„èªªæ˜ã€‚"""


def call_qwen(prompt):
    profile = session.get("profile", {})

    year = profile.get("year", "ï¼ˆæœªè¨­å®šå…¥å­¸å¹´ä»½ï¼‰")
    degree = profile.get("degree", "ï¼ˆæœªè¨­å®šå­¸ä½ï¼‰")
    college = profile.get("college", "ï¼ˆæœªè¨­å®šå­¸é™¢ï¼‰")
    dept = profile.get("dept", "ï¼ˆæœªè¨­å®šå­¸ç³»ï¼‰")
    sid = profile.get("sid", "ï¼ˆæœªè¨­å®šå­¸è™Ÿï¼‰")

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½å°å¤§å­¸ç”Ÿçš„æ³•è¦åŠ©ç†ã€‚
    å­¸ç”ŸèƒŒæ™¯å¦‚ä¸‹ï¼š
    - å…¥å­¸å¹´ä»½ï¼š{year}
    - å­¸ä½ï¼š{degree}
    - å­¸é™¢ï¼š{college}
    - å­¸ç³»ï¼š{dept}
    - å­¸è™Ÿï¼š{sid}

    è«‹ç›¡å¯èƒ½æ ¹æ“šå­¸ç”Ÿçš„èº«åˆ†çµ¦å‡ºæ›´è²¼è¿‘æƒ…æ³çš„å»ºè­°ã€‚
    """

    print(system_prompt)

    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]

    #  1. ç”¢ç”ŸèŠå¤©æ ¼å¼ promptï¼ˆç´”æ–‡å­—ï¼‰
    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    #  2. ç”¨ tokenizer ç·¨ç¢¼æˆ input_ids + attention_mask
    encoded = tokenizer(chat_prompt, return_tensors="pt").to(model.device)

    #  3. æ¨¡å‹ç”¢ç”Ÿ
    outputs = model.generate(
        input_ids=encoded["input_ids"],
        attention_mask=encoded["attention_mask"],
        max_new_tokens=512,
        do_sample=False
    )

    #  4. è§£ç¢¼çµæœ
    output_ids = outputs[0]
    response = tokenizer.decode(output_ids, skip_special_tokens=True)

    #  5. ç§»é™¤ prompt å‰ç¶´ï¼ˆå¯é¸ï¼‰
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()

    return response


#  æ¸¬è©¦ç¯„ä¾‹ï¼ˆä½¿ç”¨ BM25 æ“·å– top-k document ä½œç‚º contextï¼‰
#question = "æˆ‘ç¾åœ¨å­¸å£«ç­å¤§ä¸‰ï¼Œæ²’æœ‰è¼”ç³»ï¼Œå·²ç¶“ä¿®äº†83å­¸åˆ†ï¼Œæˆ‘é‚„å·®å¤šå°‘æ‰èƒ½ç•¢æ¥­ï¼Ÿ"
#question = "ä¸€å­¸æœŸæƒ³è¦ä¿®è¶…é25å­¸åˆ†çš„è³‡æ ¼æ˜¯ä»€éº¼ï¼Ÿ"
#retriever = BM25DocumentRetriever(pdf_folder="./ntu_rules_pdfs", corpus_path="bm25_docs_big.json")
#retriever.build_or_load_corpus()
#retriever.build_index()
#results = retriever.search(question, k=5)
#print("Top-5 documents:")
#for i, r in enumerate(results, 1):
    #preview = r["text"][:120].replace("\n", " ")
    #print(f"{i}. {r['doc_id']}  score={r['score']:.3f}  preview={preview}{'...' if len(r['text'])>120 else ''}")

#print("\nContext to feed the LLM (trimmed per doc):")
#context = retriever.build_context(question, k=5, max_chars_per_doc=2000)
#print(context[:2000])


# --------------- æ–°å¢ï¼šLLM Reranking æ¢æ–‡é¸æ®µéšæ®µ ---------------

def llm_rerank_relevant_passages(query: str, bm25_context: str) -> str:
    print(bm25_context)
    """ä½¿ç”¨åŒä¸€å€‹æ¨¡å‹ï¼Œæ ¹æ“šå•é¡Œåœ¨ BM25 context ä¸­é¸å‡ºæœ€ç›¸é—œæ¢æ–‡ã€‚"""
    rerank_prompt = f"""ä½ æ˜¯ä¸€ä½è² è²¬è³‡æ–™æ“·å–çš„åŠ©ç†ã€‚ä½ è² è²¬å¾æ–‡ä»¶ä¸­ä¿ç•™èˆ‡å­¸ç”Ÿå•é¡Œç›´æ¥ç›¸é—œçš„æ¢æ–‡æˆ–æ®µè½ã€‚å¿½ç•¥èˆ‡å•é¡Œç„¡é—œçš„å…§å®¹ã€‚ä¸è¦åŠ ä»»ä½•è§£é‡‹æˆ–åˆ†æã€‚

[å­¸ç”Ÿå•é¡Œ]
{query}

[æ–‡ä»¶]
{bm25_context}\n\n

è«‹ä»¥ 
æ ¹æ“š[Document X] pdfæª”å ç›¸é—œå…§å®¹ 
çš„æ ¼å¼ï¼Œè¼¸å‡ºèˆ‡å­¸ç”Ÿå•é¡Œæœ€ç›¸é—œçš„æ¢æ–‡æˆ–æ®µè½ã€‚
"""
    print("\n====== LLM æ¢æ–‡é¸æ®µéšæ®µ ======")
    selected_text = call_qwen(rerank_prompt)
    print(selected_text)
    return selected_text

#refined_context = llm_rerank_relevant_passages(question, context)
#prompt = generate_prompt(question, refined_context, chat_history)
#answer = call_qwen(prompt)

# æ›´æ–°æ­·å²
#chat_history.append({"user": question, "assistant": answer})

#print("ğŸ¤– å›ç­”ï¼š")
#print(answer)
#print(refined_context)




# In[10]:


#  æ¸¬è©¦ç¯„ä¾‹
#question = "å¹«æˆ‘è¦åŠƒå‰©ä¸‹çš„å­¸åˆ†è©²å¦‚ä½•ä¿®å®Œ"
#prompt = generate_prompt(question, context, chat_history)
#answer = call_qwen(prompt)

#print("ğŸ¤– å›ç­”ï¼š")
#print(answer)"""


#114
