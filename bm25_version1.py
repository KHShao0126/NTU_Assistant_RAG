#!/usr/bin/env python
# coding: utf-8

# In[2]:




# In[7]:


import os
import json
import re
from typing import List, Dict, Optional

try:
    import fitz  # PyMuPDF
except Exception as exc:  # pragma: no cover
    raise RuntimeError("PyMuPDF (fitz) is required. Please install with: pip install pymupdf") from exc


def _ensure_deps():
    """Ensure optional dependencies are available; raise helpful errors otherwise."""
    try:
        from rank_bm25 import BM25Okapi  # noqa: F401
    except Exception as exc:  # pragma: no cover
        raise RuntimeError("rank-bm25 is required. Please install with: pip install rank-bm25") from exc

    # jieba is optional; we fall back to regex if unavailable
    try:
        import jieba  # noqa: F401
    except Exception:
        pass


_ensure_deps()
from rank_bm25 import BM25Okapi


def default_tokenize(text: str) -> List[str]:
    """Tokenize text for BM25 scoring.

    Prefer jieba for Chinese segmentation; otherwise, use a regex that splits
    CJK characters to single-char tokens and Latin to word tokens.
    """
    try:
        import jieba
        return [token.strip() for token in jieba.cut(text) if token.strip()]
    except Exception:
        # Keep alphanumerics as words, split CJK into single characters
        return re.findall(r"[\u4e00-\u9fff]|[A-Za-z0-9]+", text)


class BM25DocumentRetriever:
    """Builds a BM25 index at the document (per-PDF) level and retrieves top documents.

    - Each PDF file becomes one document.
    - BM25 is built over tokenized full-document texts.
    - Use search() to get the most relevant documents for a query.
    """

    def __init__(
        self,
        pdf_folder: str = "./å°å¤§è³‡å·¥ç›¸é—œè¦ç¯„",
        corpus_path: str = "bm25_docs.json",
        tokenizer=default_tokenize,
    ) -> None:
        self.pdf_folder = pdf_folder
        self.corpus_path = corpus_path
        self.tokenize = tokenizer

        self.documents: List[Dict[str, str]] = []  # [{doc_id, text}]
        self._bm25: Optional[BM25Okapi] = None

    def build_or_load_corpus(self) -> None:
        """Load existing corpus JSON or build from PDFs and save it."""
        if os.path.exists(self.corpus_path):
            with open(self.corpus_path, "r", encoding="utf-8") as f:
                self.documents = json.load(f)
            return

        documents: List[Dict[str, str]] = []
        for filename in os.listdir(self.pdf_folder):
            if not filename.lower().endswith(".pdf"):
                continue
            pdf_path = os.path.join(self.pdf_folder, filename)
            try:
                doc = fitz.open(pdf_path)
            except Exception:
                # Skip unreadable PDFs but keep going
                continue
            text_fragments: List[str] = []
            for page in doc:
                try:
                    text_fragments.append(page.get_text())
                except Exception:
                    pass
            full_text = "\n".join(text_fragments).strip()
            if not full_text:
                continue
            documents.append({"doc_id": filename, "text": full_text})

        # Persist corpus
        with open(self.corpus_path, "w", encoding="utf-8") as f:
            json.dump(documents, f, ensure_ascii=False)

        self.documents = documents

    def build_index(self) -> None:
        """Build BM25 index for the loaded corpus."""
        if not self.documents:
            self.build_or_load_corpus()
        tokenized_docs = [self.tokenize(d["text"]) for d in self.documents]
        self._bm25 = BM25Okapi(tokenized_docs)

    def search(self, query: str, k: int = 5) -> List[Dict[str, object]]:
        """Return top-k most relevant documents by BM25 score.

        Response schema per item:
        - doc_id: str (PDF filename)
        - score: float (BM25 score)
        - text: str (full document text)
        """
        if self._bm25 is None:
            self.build_index()
        tokenized_query = self.tokenize(query)
        scores = self._bm25.get_scores(tokenized_query)
        ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]

        results: List[Dict[str, object]] = []
        for idx in ranked_indices:
            doc = self.documents[idx]
            results.append({
                "doc_id": doc["doc_id"],
                "score": float(scores[idx]),
                "text": doc["text"],
            })
        return results

    def build_context(self, query: str, k: int = 5, max_chars_per_doc: Optional[int] = 6000) -> str:
        """Build a concatenated context from top-k documents with optional per-doc trimming.

        This is useful if you want to pass entire documents (or trimmed versions) to an LLM.
        """
        top_docs = self.search(query, k=k)
        formatted_docs: List[str] = []
        for rank, item in enumerate(top_docs, start=1):
            text = item["text"]
            if isinstance(max_chars_per_doc, int) and max_chars_per_doc > 0:
                text = text[:max_chars_per_doc]
            header = f"[Document {rank}] {item['doc_id']} (score={item['score']:.3f})"
            formatted_docs.append(f"{header}\n{text}")
        return "\n\n---\n\n".join(formatted_docs)








# In[9]:


from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import json
import re

# Try to ensure BM25 is available
try:
    from rank_bm25 import BM25Okapi
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "rank-bm25"])  # silent install
    from rank_bm25 import BM25Okapi

# Simple tokenizer that works for Chinese and Latin text
try:
    import jieba
    def tokenize(text: str):
        return [t for t in jieba.cut(text) if t.strip()]
except Exception:
    def tokenize(text: str):
        return re.findall(r"[\u4e00-\u9fff]|[A-Za-z0-9]+", text)

MODEL = None
TOKENIZER = None

# è¼‰å…¥è¼ƒå°çš„ Qwen æ¨¡å‹ï¼ˆ1.8B chat ç‰ˆè¼ƒå®¹æ˜“åœ¨ 16GB RAM ä¸Šé‹è¡Œï¼‰
model_id = "Qwen/Qwen1.5-7B-Chat"

# é¸æ“‡è£ç½®ï¼ˆå„ªå…ˆ CUDAï¼Œå…¶æ¬¡ Apple MPSï¼Œæœ€å¾Œ CPUï¼‰
if torch.cuda.is_available():
    device = "cuda"
elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

# æ¨¡å‹å¿«å–
if MODEL is None or TOKENIZER is None:
    print("Loading Qwen model and tokenizer (this may take a few minutes the first time)...")
    TOKENIZER = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    MODEL = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16 if device in ("cuda", "mps") else torch.float32
    ).to(device)
else:
    print("Using cached Qwen model and tokenizer.")

tokenizer = TOKENIZER
model = MODEL

# å»ºç«‹æ¨ç†ç®¡ç·šï¼ˆå¯é¸ï¼Œä¸ä¸€å®šè¦ä½¿ç”¨ï¼‰
llm = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=False)







def generate_prompt(question, context):
    return f"""ä½ æ˜¯ä¸€ä½å°å¤§è³‡å·¥ç³»çš„æ³•è¦åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å­¸ç”Ÿçš„å•é¡Œã€‚

[æ³•è¦è³‡æ–™]
{context}

[å­¸ç”Ÿå•é¡Œ]
{question}

è«‹çµ¦å‡ºæº–ç¢ºã€æ¸…æ¥šçš„å›è¦†ï¼Œè‹¥è³‡æ–™ä¸è¶³ï¼Œè«‹èªªæ˜é‚„éœ€è¦å“ªäº›å­¸ç”Ÿè³‡è¨Šã€‚"""


def call_qwen(prompt):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯å°å¤§è³‡å·¥ç³»çš„æ³•è¦åŠ©ç†ï¼Œè«‹æ ¹æ“šè³‡æ–™å›ç­”å•é¡Œã€‚ æå‡ºå•é¡Œçš„éƒ½æ˜¯å°å¤§è³‡å·¥çš„å­¸ç”Ÿ"},
        {"role": "user", "content": prompt}
    ]

    # âœ… 1. ç”¢ç”ŸèŠå¤©æ ¼å¼ promptï¼ˆç´”æ–‡å­—ï¼‰
    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # âœ… 2. ç”¨ tokenizer ç·¨ç¢¼æˆ input_ids + attention_mask
    encoded = tokenizer(chat_prompt, return_tensors="pt").to(model.device)

    # âœ… 3. æ¨¡å‹ç”¢ç”Ÿ
    outputs = model.generate(
        input_ids=encoded["input_ids"],
        attention_mask=encoded["attention_mask"],
        max_new_tokens=512,
        do_sample=False
    )

    # âœ… 4. è§£ç¢¼çµæœ
    output_ids = outputs[0]
    response = tokenizer.decode(output_ids, skip_special_tokens=True)

    # âœ… 5. ç§»é™¤ prompt å‰ç¶´ï¼ˆå¯é¸ï¼‰
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()

    return response

# âœ… æ¸¬è©¦ç¯„ä¾‹ï¼ˆä½¿ç”¨ BM25 æ“·å– top-k chunk ä½œç‚º contextï¼‰
question = "æˆ‘ç¾åœ¨å­¸å£«ç­å¤§ä¸‰ï¼Œæ²’æœ‰è¼”ç³»ï¼Œå·²ç¶“ä¿®äº†83å­¸åˆ†ï¼Œæˆ‘é‚„å·®å¤šå°‘æ‰èƒ½ç•¢æ¥­ï¼Ÿ"
retriever = BM25DocumentRetriever(pdf_folder="./å°å¤§è³‡å·¥ç›¸é—œè¦ç¯„", corpus_path="bm25_docs.json")
retriever.build_or_load_corpus()
retriever.build_index()
results = retriever.search(question, k=5)
print("Top-5 documents:")
for i, r in enumerate(results, 1):
    preview = r["text"][:120].replace("\n", " ")
    print(f"{i}. {r['doc_id']}  score={r['score']:.3f}  preview={preview}{'...' if len(r['text'])>120 else ''}")

print("\nContext to feed the LLM (trimmed per doc):")
context = retriever.build_context(question, k=5, max_chars_per_doc=2000)
print(context[:2000])
prompt = generate_prompt(question, context)
answer = call_qwen(prompt)

print("ğŸ¤– å›ç­”ï¼š")
print(answer)


# In[10]:


# âœ… æ¸¬è©¦ç¯„ä¾‹
question = "æˆ‘éœ€è¦ä¿®å“ªå¹¾é¡çš„é€šè­˜æ‰å¯ä»¥ç•¢æ¥­"
prompt = generate_prompt(question, context)
answer = call_qwen(prompt)

print("ğŸ¤– å›ç­”ï¼š")
print(answer)

